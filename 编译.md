## InferenceHelper的下载

打开一个Git bash窗口参照[InferenceHelper_Sample](https://github.com/iwatake2222/InferenceHelper_Sample)页面描述方式将代码下载下来：
~~~bash
# 下载Sample代码
git clone https://github.com/iwatake2222/InferenceHelper_Sample
cd InferenceHelper_Sample
git submodule update --init
# 下载子模块代码已经预编译的文件
sh InferenceHelper/third_party/download_prebuilt_libraries.sh
# (Optioal)下载资源文件，主要是图片，模型文件等
sh ./download_resource.sh
~~~

由于国内IP连接Github上的tensorflow等项目十分慢，甚至无法下载，可使用[Google Colab](https://colab.research.google.com/drive/1R6t0Cj0pDGdIpFdT9tZumWYrTAgR0HC6)帮助下载，先加载到Google的云服务器上，然后再将`InferenceHelper_Sample`打包下载。

## 编译
在Linux系统上，可直接使用demo的编译指令进行(未测试)：
~~~bash
cd pj_cls_mobilenet_v2
mkdir -p build && cd build
cmake .. -DINFERENCE_HELPER_ENABLE_MNN=on
make
./main
~~~

在Windows上可借助`Cmake-gui`工具进行编译
 - `Where is the source code`: 放需要编译的项目，如`path-to-InferenceHelper_Sample/pj_cls_mobilenet_v2`
 - `Where to build the binaries` : 放编译文件的地址，如`path-to-InferenceHelper_Sample/pj_cls_mobilenet_v2/build`
 - 然后点击`Configure`，在可选参数中选择需要的，如使用tflite，则需要勾选`INFERENCE_HELPER_ENABLE_TFLITE`
 - 点击`Generate`，下方显示生成成功以后，可以`Open Project`打开项目。生成项目即可得到`main.exe`可执行文件
 - 将`main`项目设置为启动项目，则可以对项目进行调试

原仓库中要求OpenCV版本>4.0，Visual Studio 2019。实际使用中发现使用OpenCV==3.4.1也是可以的，VS2015版本也是适用的。

## Inference

以下是InferenceHelper进行推理的大致流程，由于是直接冲代码片段中截取的，会比较乱，主要用于理解流程，具体实现还是建议直接找一个project看了源码后改。
查看了一些Demo发现，一般会创建一个`xxxEngine`的类，通过预编译指令定义TENSORTYPE, MODEL_NAME等信息：
~~~
#define TENSORTYPE  TensorInfo::kTensorTypeFp32
#define MODEL_NAME  "mobilenetv2-1.0.onnx"
#define INPUT_NAME  "data"
#define INPUT_DIMS  { 1, 3, 224, 224 }
#define IS_NCHW     true
#define IS_RGB      true
#define OUTPUT_NAME "mobilenetv20_output_flatten0_reshape0"
~~~
定义3个基本变量：
~~~cpp
    std::unique_ptr<InferenceHelper> inference_helper_; // 推理模型
    std::vector<InputTensorInfo> input_tensor_info_list_; // 输入节点的list
    std::vector<OutputTensorInfo> output_tensor_info_list_; // 输出节点的list
~~~

设置输入节点：
~~~cpp
    input_tensor_info_list_.clear();
    InputTensorInfo input_tensor_info(INPUT_NAME, TENSORTYPE, IS_NCHW);
    input_tensor_info.tensor_dims = INPUT_DIMS;
    input_tensor_info.data_type = InputTensorInfo::kDataTypeImage;
    input_tensor_info.normalize.mean[0] = 0.485f;  
    input_tensor_info.normalize.mean[1] = 0.456f;
    input_tensor_info.normalize.mean[2] = 0.406f;
    input_tensor_info.normalize.norm[0] = 0.229f;
    input_tensor_info.normalize.norm[1] = 0.224f;
    input_tensor_info.normalize.norm[2] = 0.225f;
    input_tensor_info_list_.push_back(input_tensor_info);
~~~
设置输出节点：
~~~cpp
    output_tensor_info_list_.clear();
    output_tensor_info_list_.push_back(OutputTensorInfo(OUTPUT_NAME, TENSORTYPE));
~~~
然后定义模型的类型，这里具体的选择也是通过预编译指令进行的：
~~~cpp
#if defined(INFERENCE_HELPER_ENABLE_OPENCV)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kOpencv));
#elif defined(INFERENCE_HELPER_ENABLE_TFLITE)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorflowLite));
#elif defined(INFERENCE_HELPER_ENABLE_TFLITE_DELEGATE_XNNPACK)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorflowLiteXnnpack));
#elif defined(INFERENCE_HELPER_ENABLE_TFLITE_DELEGATE_GPU)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorflowLiteGpu));
#elif defined(INFERENCE_HELPER_ENABLE_TFLITE_DELEGATE_EDGETPU)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorflowLiteEdgetpu));
#elif defined(INFERENCE_HELPER_ENABLE_TFLITE_DELEGATE_NNAPI)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorflowLiteNnapi));
#elif defined(INFERENCE_HELPER_ENABLE_TENSORRT)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kTensorrt));
    if (inference_helper_) {
        InferenceHelperTensorRt* p = dynamic_cast<InferenceHelperTensorRt*>(inference_helper_.get());
        if (p) {
            p->SetDlaCore(-1);  /* Use GPU */
        }
    }
#elif defined(INFERENCE_HELPER_ENABLE_NCNN)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kNcnn));
#elif defined(INFERENCE_HELPER_ENABLE_MNN)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kMnn));
#elif defined(INFERENCE_HELPER_ENABLE_SNPE)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kSnpe));
#elif defined(INFERENCE_HELPER_ENABLE_ARMNN)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kArmnn));
#elif defined(INFERENCE_HELPER_ENABLE_NNABLA)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kNnabla));
#elif defined(INFERENCE_HELPER_ENABLE_NNABLA_CUDA)
    inference_helper_.reset(InferenceHelper::Create(InferenceHelper::kNnablaCuda));
#else
    PRINT_E("Inference Helper type is not selected\n");
#endif
~~~

对模型进行初始化：
~~~cpp
    if (inference_helper_->SetNumThreads(num_threads) != InferenceHelper::kRetOk) {
        inference_helper_.reset();
        return kRetErr;
    }
    if (inference_helper_->Initialize(model_filename, input_tensor_info_list_, output_tensor_info_list_) != InferenceHelper::kRetOk) {
        inference_helper_.reset();
        return kRetErr;
    }
~~~

推理的过程调用`inference_helper_->Process()`方法即可，然后就可以在output_tensor_info_list_中获取到结果。可以通过以下的方式将结果返回到一个`std::vector`中，方便后续进行操作。
